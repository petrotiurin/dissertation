Cover Page.

Declaration.

	This dissertation is submitted to the University of Bristol in accordance with the requirements of the degree of MEng in the Faculty of Engineering. It has not been submitted for any other degree or diploma of any examining body. Except where specifically acknowledged, it is all the work of the Author.

Contents.

-Context
-Technical Background


List of figures/tables/algorithms.

Executive summary (1 page).

	This section should precis the project context, aims and objectives, and main contributions and achieve- ments; the same section may be called an abstract elsewhere. The goal is to ensure the reader is clear about what the topic is, what you have done within this topic, and what your view of the outcome is.
	The former aspects should be guided by your specification: essentially this section is a (very) short version of what is typically the first chapter. The latter aspects should be presented as a concise, factual bullet point list. 


	This thesis investigates the solutions for efficient file transfer between a client and a server during an unstable network connection. In particular, CouchDB is used, as it makes use of client-server communication but lacks robustness in the file transfer implementation.

	Currently, CouchDB and most of the other open-source databases can receive and store files as well as structured data. However, the protocols governing the transfer of these files are quite simplistic. Nowadays, the use of mobile devices is prevailing and their storage space is rapidly growing. Big files, like high-definition videos, are being transferred from and to them. However, the network connection they use is not always perfect. A need arises of a new protocol that would minimise the time and traffic used and ensure the successful delivery of large files over the changing or unstable network conditions.

	This project focuses on developing an algorithm that would be able to transfer files of various sizes to and from a CouchDB instance while allowing to pause and resume the transfer, and recover from packet loss during the transfer without the need to restart the transmission from the beginning.

	The main contributions are:

	* An algorithm specification that resolves the said problem and allows resumable file transfer.

	* A Java client and proxy server implementation, which implement the abovementioned algorithm and can be modified to the need of any database implementation.

	* A native CouchDB applicatio of the algorithm.

	* Benchmark on both proxy server and native implementations and comparison between them.


Supporting technologies (1 page).

	This section should present a detailed summary, in bullet point form, of any third-party resources (e.g., hardware and software components) used during the project. Use of such resources is always perfectly acceptable: the goal of this section is simply to be clear about how and where they are used, so that a clear assessment of your work can result.

	* Apache Tomcat server was used in order to test the Java proxy server developed.
	* Apache CouchDB was used as an example database which used the native implementation of the algorithm.

Notation and acronyms (optional, 1-2 pages).

Acknowledgements (1 page).

	Advisor, supervisor, mum, dad, etc.

Contextual Background (10 pages).

	This chapter should describe the project context, and motivate each of the proposed aims and objectives. Ideally, it is written at a fairly high-level, and easily understood by a reader who is technically competent but not an expert in the topic itself.
	In short, the goal is to answer three questions for the reader. First, what is the project topic, or problem being investigated? Second, why is the topic important, or rather why should the reader care about it? For example, why there is a need for this project (e.g., lack of similar software or deficiency in existing software), who will benefit from the project and in what way (e.g., end-users, or software developers) what work does the project build on and why is the selected approach either important and/or interesting (e.g., fills a gap in literature, applies results from another field to a new problem). Finally, what are the central challenges involved and why are they significant?
	The chapter should conclude with a concise bullet point list that summarises the aims and objectives.
	

	Database technology, as a means of storing organized data, is at the core of many systems and services employed in the industry nowadays. Databases are used in conjunction with Database Management Systems (DBMS), which are designed to allow creation, querying, deletion and administration of them. In 1970 Edgar Codd outlined an approach to database construction that was based on relational model. In his paper he suggested a model where a database would consist of interlinked tables of fixed-length records of data, each table used for different type of entity. In this model some bit of information would be used as a “key” to uniquely identify a particular record in different tables. Optional elements would be moved out of the main table where it could then be found by searching with this key.

	Relational model has been the main approach to the database design and implementation until late 2000’s. Up to that point this model was enough to satisfy the industry needs. However, with the increased availability of the internet-connected devices the new industry-wide needs arose. The main trends became: supporting the large number of concurrent users, ability to incorporate semi-structured and unstructured data (e.g. log files, tweets, audio and video), and scaling capabilities. The latter became especially relevant with the rise of cloud computing, where the ability to scale-out with the periodic or spiked loads is crucial. Because of the centralized, share-everything technology of the relational model it scales up (requires a better server, with more CPUs, memory, etc.), rather then out (more servers of the same configuration) and therefore it becomes problematic to use with the applications that demand easy and dynamic scalability.  In order to retrieve data from a relational database the desired information has to be collected from many tables (in case of modern enterprise applications) and combined together before it could be accessed by the application. This became a problem for modern big data applications.

	In 2009 a new approach to database design was invented. The NoSQL idea was introduced, which referred to a movement in database design that didn’t use the conventional relational model. Instead, depending on the implementation, the data was stored in key-value pairs, documents or graphs. For example, a document-oriented NoSQL database takes the data to store and aggregates it into a document using a specified format (e.g. JSON in case of CouchDB). A single document might contain combined data that would span over 20 tables in a relational database. This might lead to duplication, however the storage cost tends to get cheaper and often gets outweighed by the performance advantages. Also, NoSQL databases are designed to be schemaless, which means new types of data can easily be added to documents without application disruption.

	Another prominent trend that had a major impact on the industry is the rise of mobile devices. Mobile phones, tablets and laptops are becoming more popular as the advancements in chip design and manufacturing of these devices decrease their cost and increase the variety. According to comScore mobile usage report number of global users of mobile devices has overtaken the number of users of desktop computers.  Cisco Mobile Data Traffic Forecast states that popularity of the mobile devices continues to rise in terms of data traffic. Over one year, in 2014, the mobile traffic grew by 69% as well as smartphone usage grew by 45%. These figures are forecasted to grow even further by 2019. For example, the report projects that the global mobile traffic will grow nearly tenfold in that period. Another important point mentioned in the report is the dominance of the video traffic in the data transfer – it exceeds 50% of the total mobile traffic.

	However, despite such success of mobile devices their network connectivity is not always the same as their desktop counterparts. The speed, reliability and latency can widely vary depending on the location, environment conditions, distance from the connection point and other factors. Therefore, the developers aiming for mobile market need to consider more efficient transfer protocols. This is especially relevant when mobile operators limit the amount of data that can be used or charge on the amount of data used, as in this case the users want as little network as possible used to complete the task.

	Many NoSQL systems support access by mobile devices. They also allow users to upload external files as attachments to the database entry. In terms of mobile devices these can be videos or pictures. These files can be up to hundreds of megabytes and therefore efficient transfer of them is crucial. Out of the most popular NoSQL database implementations only MongoDB has a specification for storing and retrieving large files from the database called GridFS. However, it is more of a convention of how to store large files, rather than a native implementation. The purpose of this dissertation is to investigate the possible adaptation of a similar file-transfer algorithm to a wider variety of database implementations. 



	TODO: Divide into sub-sections:
		-Introduction and choice of topic (noSQL popularity and mobile networks)
		-The problem of file transfer
		-Objectives?

Technical Background (10 to 20 pages).
	
	This chapter is intended to describe the technical basis on which execution of the project depends. The goal is to provide a detailed explanation of the specific problem at hand, and existing work that is relevant (e.g., an existing algorithm that you use, alternative solutions proposed, supporting technologies).
	Per the same advice in the handbook, note there is a subtly difference from this and a full-blown literature review (or survey). The latter might try to capture and organise (e.g., categorise somehow) all related work, potentially offering meta-analysis, whereas here the goal is simple to ensure the dissertation is self-contained. Put another way, after reading this chapter a non-expert reader should have obtained enough background to understand what you have done, and then accurately assess your work. You might view an additional goal as giving the reader confidence that you are able to absorb, understand and clearly communicate highly technical material.

	1. NoSQL?

	2. CouchDB

	CouchDB is a NoSQL database that has no schema or pre-defined data-structures such as tables. Data in the database is stored in JSON document(s) and therefore the structure of the data (or document) can change depending on the needs.

	2.1 JSON

	JSON is a format that uses human-readable text to transmit data as attribute-value pairs. It is primarily used to transmit data between a server and a web-application. JSON is a language-independent format and many languages have built-in tools to generate and parse such data. 

	JSON accepts various common data types such as Number, String, etc. It also has more complex data types such as Object, which stores an unordered collection of key/value pairs, Array, which stores a list of any other values, and null, which is an empty value.

	2.2 Views

	CouchDB has a special tool designed to query this semi-structured data called view. Views are specified in JavaScript and are the method of aggregating and reporting on the documents in a database. They are built on-demand to aggregate, join and report on database documents. Views are built dynamically and don’t affect the underlying document and there can be many different view representations of the same data.

	2.3 Replication

	CouchDB is a distributed database system. Any number of CouchDB hosts (servers and offline-clients) can have independent “replica copies” of the same database, where applications have the full set of database-interaction features (query, add, edit, delete). When back online or on a schedule, database changes can be replicated bi-directionally. Replication provides redundancy and increases data availability. With multiple copies of the database on separate servers, replication protects a database from the loss of a single server. 

	There are two main replication methods: master to slave and master to master. In the first case there is an original (master) database and its copies (slaves). The changes are submitted to the original and are propagated to its copies. In the second case the updates can be submitted to any database node and will be propagated to all the other nodes. This case is usually more desirable, but it introduces some challenges, like conflicts.

	CouchDB supports master to master replication and therefore sometimes, when database is replicated, a conflict can arise if the same document was changed in both source and destination. CouchDB provides conflict-detection functionality as well as incremental replication, which copies only the documents that were changed since previous replication. 

	3. Related work

	3.1 GridFS for MongoDB

	Similarly to CouchDB, MongoDB is a document-oriented NoSQL database. It uses BSON format of storing data, which is similar to JSON, but is encoded in binary and therefore is not human-readable on its own. Unlike CouchDB, it uses master to slave replication method.

	MongoDB supports binary files as attachments to documents, but limits their size to 16MB. In order to add files larger than 16MB GridFS specification was developed. Instead of storing file in a single document it divides a file into chunks and stores them as a separate document. Another document holds file metadata such as number of chunks, their length, etc. Such file can then be queried using GridFS to retrieve it all or parts of it, by performing range queries.

	3.2 How is this project different?

	While this project is using a similar algorithm to GridFS, it expands on several of its features. The main aim of this project is to create a reliable transfer protocol that can be used to efficiently transfer large file by mobile devices, therefore the differences are:

	•	Project’s proposed algorithm and it’s implementation are designed to tackle the problem of reliable transfer of data and resume the transmission from the point of failure without the need to restart the whole transfer from the beginning. GridFS has no special functionality to be able to cope with network failures.
	•	The implementation is packet and data loss resistant, as it makes sure that each file chunk is received by the server and is intact. The algorithm retries to send the chunk until the acknowledgment from the server arrives. The server does not acknowledge the transfer until the checksum of the data chunk matches the supplied checksum sent by a client.
	•	The transfer can be paused and resumed at any time. This is especially relevant for mobile users, who can use this feature in order to temporarily decrease the network load to use other bandwidth-dependent applications.
	•	A separate implementation of the proposed algorithm in form of a proxy serve is developed in order to be used with any of the similar NoSQL database implementations with minor changes needed to the original implementation. It also can be used with older CouchDB versions that don’t have the proposed chunked transfer functionality. 


Project Execution (20 pages).

	This chapter is intended to describe what you did: the goal is to explain the main activity or activities, of any type, which constituted your work during the project. The content is highly topic-specific, but for many projects it will make sense to split the chapter into two sections: one will discuss the design of something (e.g., some hardware or software, or an algorithm, or experiment), including any rationale or decisions made, and the other will discuss how this design was realised via some form of implementation.
	This is, of course, far from ideal for many project topics. Some situations which clearly require a different approach include:
	• In a project where asymptotic analysis of some algorithm is the goal, there is no real “design and implementation” in a traditional sense even though the activity of analysis is clearly within the remit of this chapter.
	• In a project where analysis of some results is as major, or a more major goal than the implementation that produced them, it might be sensible to merge this chapter with the next one: the main activity is such that discussion of the results cannot be viewed separately.
	Note that it is common to include evidence of “best practice” project management (e.g., use of version control, choice of programming language and so on). Rather than simply a rote list, make sure any such content is useful and/or informative in some way: for example, if there was a decision to be made then explain the trade-offs and implications involved.

	1.	Algorithm design
	1.1	Client algorithm
		*Subject to minor changes*
		The abstract algorithm goes as follows:
		1.	Send a request to the server to start the transfer, containing the document ID of a destination document.
		2.	Wait for server acknowledgment. If an error received or timed out go to 1.
		3.	Construct a stack with chunk numbers for current file.
		4.	For MaxN iterations:
			4.1.	Pop a number N from the stack.
			4.2.	Construct an asynchronous task that would send data from position N*ChunkLength to (N+1)*ChunkLength in the file.
			4.3.	Commence execution of the task.
		5.	While stack is not empty:
			5.1.	If one of the tasks was finished:
				5.1.1.	If the task succeeded:
					5.1.1.1.	Pop a number N from the stack.
					5.1.1.2.	Construct an asynchronous task that would send data from position N*ChunkLength to (N+1)*ChunkLength in the file.
					5.1.1.3.	Commence execution of the task.
				5.1.2.	If the task failed:
					5.1.2.1.	Restart the execution of this task.
			5.2.	If pause signal received:
				5.2.1.	Abort all the tasks.
				5.2.2.	Wait for resume signal.
		6.	Wait for all tasks to finish. If any fail – restart them. 
		7.	Send the final request to indicate that the transfer is finished.
		8.	Wait for server acknowledgment. If an error received or timed out go to 6.

		Explanation:
		In the beginning the algorithm makes sure the server is ready to accept the file chunks. The algorithm then constructs a stack, which will hold the chunk numbers of chunks that haven’t been sent. 
		For the data transfer the algorithm uses asynchronous tasks that are managed by the system. These tasks read a specified part of the file in the memory, construct an HTTP request, send it and wait for the server response. The task fails if the server returned failure or if the timeout is reached. It succeeds in an acknowledgement received. 
		A set number of tasks is initiated in the beginning by popping chunk numbers from the stack and passing the appropriate byte ranges to be read from the file to each task.  Then, while the stack is not empty, the algorithm replaces any finished tasks with new. If the finished task succeeded a new chunk number is popped from the stack and a task instantiated. If the finished task resulted in a failure it’s restarted.
		When the stack is emptied the algorithm waits for all tasks to finish and restart them if necessary. The final request is sent in the end in order to complete the transfer and add the resulting file to a document.
	
	1.2	Chunk format
		Each chunk has a payload and several headers:
			•	“Chunked-Transfer” – can be set to “true” to specify that current request is a part of series and should not be treated on its own, or “final” to end the upload and attach the file to a specified document.
			•	“Start” – the start position of a chunk in the file. Value in bytes.
			•	“MD5” – md5 checksum of the chunk, calculated by the client.
			•	“DocID”. Proxy server only. An ID of a destination document for the file. Since document ID in CouchDB is specified in the URL this header is omitted in native implementation. 

	1.3	

	
	2.	Implementation

		The proposed implementation was developed in two forms: as a proxy server that doesn’t require any changes to the underlying database, and a native implementation to CouchDB, which allows the database to directly accept the chunked file transfer.
		Each implementation consists of a client side and a server side.  
	
	2.1	Client

		A client application is designed to be able to transmit files using the proposed algorithm. It is developed in Java, as it stays the most popular programming language in the industry and can be run on most computers and operating systems. This language is also chosen as it is used for Android application development. Android is one of the leading operating systems on the mobile market and therefore the implementation of the client in Java allows for it to be easily ported to this platform to cover a large proportion of mobile users.

	2.1.1	Preparing for transmission

		The implementation follows the algorithm closely, but takes into account the specifics of the Java language. At the startup the program needs an address of the server, a file to transmit and a chunk size to be specified. Execution starts at the main thread, where the program sets the number of helper threads. By default this number is set to 4, as it is the most common number of CPU cores in a modern computer and therefore it leads to better performance on such machines. The user can also modify it during the startup to allow adjusting it to a specific machine or need.
		The threads are managed by a ‘ListeningExecutorService’ implemented in Google Guava library. Standard Java ‘ExecutorService’ allows queuing tasks to be executed by threads. It automatically adds the queued tasks from its task pool to the idle threads. New tasks can be added to the thread pool at any time. ‘ListeningExecutorService’ is build-up on the standard ‘ExecutorService’ that allows adding code to be executed on the success (or failure) of these tasks. For our purpose, at the end of a task it creates a new one if there are chunks on the stack. Also, a CountDownLatch is used to wait for all threads to finish. Its initial value is equal to the number of simultaneous tasks, when a task terminates without creating a new one it decreases the latch count. This allows avoiding constantly checking if all of the tasks are done on the main thread and therefore decrease the CPU load.
		After the start-up, the client sends an empty HTTP PUT request, with the destination document ID in its header. It waits for the server response for a time defined by the timeout variable and repeats the request if a code different from 200 is received, or request times out. This makes sure the server is available for the file transmission.
		
	2.1.2	File transmission
	
		When initial request succeeds the client starts the file transfer. It creates a stack to hold file chunks that weren’t transmitted yet. Initially the stack is filled with a total number of chunks that split the given file. During the start the program pops a first few chunk numbers from the stack. The number of chunks is suggested to be equal to twice the number of threads [TEST]. The implementation then proceeds to create an asynchronous task for each chunk range popped from the stack and add them to the execution pool. The number of tasks allows minimizing the time when any thread is idle, as the system will automatically assign a new task out of the task pool when the previous task is finished. It also avoids overloading the memory (which is crucial for mobile devices), as only a limited number of tasks are stored at any time. 
	
	2.1.3	Asynchronous task
	
		Asynchronous task is implemented as a class, which inherits from the standard Java Runnable class so it can be passed to the execution pool. The code reads the supplied byte range from the file using the FileChannel construct. This class allows the parallel access to a file as well as reading from an arbitrary location in it. The task constructs an HTTP PUT request to the server containing the data read from the file as a payload, destination document ID and the byte range of the data in the file as headers. The request is sent and if it succeeds the task is finished - creates a new task to replace it if the stack is not empty or just terminates otherwise. In case the request fails, either by timeout or the server returns failure it adds its chunk number back to the stack and creates a new task.

	2.1.4	Ending the transmission

		The main thread waits for all the helper threads to finish before proceeding by the means of ‘await’ function of CountDownLatch. When all the tasks have finished the latch value becomes zero and it allows the thread to proceed sending the finish request. It is an HTTP PUT request containing no payload, but a header with the destination document ID and “Chunked-Transfer” header value set to “final”. Similarly with the initial request the client repeats this request if it times out or server returned an error. There is a special case, when the server response was lost and the server received the next request when the file was already attached to the document. In this case the client verifies if the file was successfully added by querying the document in the database. When the transmission was verified the transfer program execution ends.
	
	2.2	Proxy server

		This implementation is designed to work without any special changes to the underlying database. It is also made to be easily adaptable to other database implementations similar to CouchDB. A server is designed to accept the chunks from the client, combine them into original file and transmit it to the database, adding it as attachment to a specified document. It is advised to set up the proxy server on the same network as the underlying database, or with a reliable connection to it. This is due to the fact that the proposed algorithm is used to transfer the file to the server, but after the assembly the server transfers the file normally, as a single HTTP request.
		The server is written in Java due to the cross-platform nature of this language as well as its popularity. The main class extends an HttpServlet class, which is a standard Java class to be used in web server applications. It has functions to handle PUT and GET requests. Former to handle receiving file chunks from clients and latter for transmitting chunks to them.
		When the server receives a PUT request with the “Chunk-Transfer” header set to “initial” it creates a file in a temporary directory with the name consisting of document ID and prefix “in_”. When a PUT request with the header value set to “true” is received it uses a ‘FileChannel’ class supplied by Java standard library to write data to an arbitrary position in the file. If a PUT request is received with the header value of “final” the server changes a prefix of the stored file to “out_” and starts an asynchronous task that transmits the file to the database. The reason for this renaming of the file is due to some requests being delayed.

	2.3	Native implementation

		Since CouchDB is an open-source database its code is in public domain and can be modified. The latest code for CouchDB was downloaded from the GitHub code hosting service and modifications were made.
		CouchDB is written in Erlang, which is a functional language and therefore highly differs from object-oriented Java. This led to the receiving code being different from the proxy server discussed above. CouchDB contributor community follows strict coding guidelines so locating the right place for the patch was fairly simple.
		The implementation was added to a function that handles the attachment requests. This function initially was parsing the supplied request body and headers and, depending on the request type, deleted or created a new attachment. In order to allow the chunked file upload another check was added in the beginning, which would check if the “Chunked-Transfer” header is present. The additional code would then be executed only if this header is found, allowing the database to function normally in other cases. If the “Chunked-Transfer” header has an “initial” value then a new file in the temporary directory is created. Its name is a concatenation of the document ID and filename for quick access. Multiple transfers of different files can be run simultaneously due to the concurrent nature of Erlang. For each new document ID/filename pair a new file is created. When the header has a “true” value the database receives the payload – file chunks. A built-in function ‘file:pwrite’ is used, which allows concurrent write to a single file in arbitrary position. Therefore, incoming requests can be processed in concurrent fashion for better performance. If a request with “Chunk-Transfer” header set to “final” is received the database adds the file to the specified document and deletes it from the temporary directory.
	
	2.4 Experimental Setup

		The implementation was tested on a personal laptop running MacOS 10.9.5. All the components were run locally. An instance of CoudhDB was set up and a test database created. A test client was written that would use the algorithm implementation in order to transmit the specified file. During the testing of the proxy server it also had the code for receiving files. “Slowy” software was used in order to simulate the unstable network conditions. 
	
	2.5	Experiments

		Efficiency of an algorithm is dictated by its usage of system resources. It can be CPU usage, memory, or time the algorithm takes to complete. In the following experiments I try to measure how well my algorithm performs given different parameters as well as network conditions.

	2.5.1	CPU usage

		The initial implementation of the algorithm was using standard Java ExecutorService class. New tasks were assigned by the main thread on completion of any of the running tasks. In order to check if a task has finished its job the main thread had to continuously call each tasks isDone() method. This would load the CPU regardless of the processing done by the tasks.
		The new implementation uses listeners to create new tasks on completion of old ones. This leads to the main thread being idle while the threaded tasks do the same amount of work. This improvement not only decreased the overall execution time of the algorithm, but also allowed lower CPU usage in certain cases.
		During the perfect network conditions, despite the reduction in execution time, the CPU load remains high even with the new code in place. This is due to the fact that the transfer of a single chunk through such network is nearly instantaneous and therefore the tasks are created at a rate dictated by the CPU speed.
		When the faulty network conditions are introduced the CPU load decreases accordingly. The tasks would need to wait for the request to be completed (or rejected by timeout) before proceeding with creating a new task. During this wait time the thread is idle and therefore it leads to a decrease of average CPU usage. 

	2.5.2	Chunk size

		I believe the size of the chunks a file is split into is the main deciding factor that affects the efficiency of this algorithm. Bigger chunks lead to more data being transferred in a single request therefore making it more “pricy” if the request fails. Because chunks are re-sent after they fail, if a transfer of a larger chunk is interrupted more data has to be sent again than in case of a smaller chunk.
		I have run some tests to verify this belief as well as test the time efficiency of the algorithm.  The client implementation was tested against a single request sent with CURL. CURL is a command line tool for transferring data using various protocols, in our case HTTP. At first, a base case was tested when there is no packet loss and the network speed is only limited by the speed of writing data to disk. Slowy software was turned off and all the components were run locally on the same machine. The series of conducted tests showed clear superiority of transferring files with CURL rather than the proposed implementation (Figure []). This is to be expected, as with the perfect network conditions the fastest way to transfer a file would be in one go. Any additional processing would only increase the time. This is also proven by chunk size test (Figure []), where the implementation was run with different chunk sizes. For both file sizes there is a clear trend of the algorithm execution time going down with the increase of the chunk size.

	2.4.3	Timeout value

		There are two timeout parameters set during the HTTP connection establishment phase of asynchronous tasks: connect and read timeouts. Connect timeout is the maximum time allowed to complete the initial TCP handshake. Read timeout is the time allowed to wait for the data to be received, specifically if the server fails to send a byte after the specified time the timeout occurs.
		The timeout value has to be carefully specified in order to maximise the algorithm effectiveness. In a fast network but with a high packet loss a small timeout value would benefit the execution time, as the program would wait less for dropped connections. However, in a slow but more reliable network this might lead to the program disregarding connections that are still transferring data (or being established) and therefore decrease the performance.  


Critical Evaluation (10 pages).

	This chapter is intended to evaluate what you did. The content is highly topic-specific, but for many projects will have flavours of the following:
	1. functional testing, including analysis and explanation of failure cases,
	2. behavioural testing, often including analysis of any results that draw some form of conclusion wrt. the aims and objectives, and
	3. evaluation of options and decisions within the project, and/or a comparison with alternatives.
	This chapter often acts to differentiate project quality: even if the work completed is of a high technical quality, critical yet objective evaluation and comparison of the outcomes is crucial. In essence, the reader wants to learn something, so the worst examples amount to simple statements of fact (e.g., “graph X shows the result is Y”); the best examples are analytical and exploratory (e.g., “graph X shows the result is Y, which means Z; this contradicts [1], which may be because I use a different assumption”). As such, both positive and negative outcomes are valid if presented in a suitable manner.


Conclusion (2 pages).
	
	The concluding chapter of a dissertation is often underutilised because it is too often left too close to the deadline: it is important to allocation enough attention. Ideally, the chapter will consist of three parts:
	1. (Re)summarise the main contributions and achievements, in essence summing up the content.
	2. Clearly state the current project status (e.g., “X is working, Y is not”) and evaluate what has been achieved with respect to the initial aims and objectives (e.g., “I completed aim X outlined previously, the evidence for this is within Chapter Y”). There is no problem including aims which were not completed, but it is important to evaluate and/or justify why this is the case.
	3. Outline any open problems or future plans. Rather than treat this only as an exercise in what you could have done given more time, try to focus on any unexplored options or interesting outcomes (e.g., “my experiment for X gave counter-intuitive results, this could be because Y and would form an interesting area for further study” or “users found feature Z of my software difficult to use, which is obvious in hindsight but not during at design stage; to resolve this, I could clearly apply the technique of Smith [7]”).

	1. Main achievements
	2. Further study.

Bibliography.

Appendices.

	Content which is not central to, but may enhance the dissertation can be included in one or more appendices; examples include, but are not limited to
	• lengthy mathematical proofs, numerical or graphical results which are summarised in the main body,
	• sample or example calculations, and
	• results of user studies or questionnaires.
	Note that in line with most research conferences, the marking panel is not obliged to read such appendices.