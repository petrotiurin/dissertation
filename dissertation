Cover Page.

Declaration.

	This dissertation is submitted to the University of Bristol in accordance with the requirements of the degree of MEng in the Faculty of Engineering. It has not been submitted for any other degree or diploma of any examining body. Except where specifically acknowledged, it is all the work of the Author.

Contents.

-Context
-Technical Background


List of figures/tables/algorithms.

Executive summary (1 page).

	This section should precis the project context, aims and objectives, and main contributions and achieve- ments; the same section may be called an abstract elsewhere. The goal is to ensure the reader is clear about what the topic is, what you have done within this topic, and what your view of the outcome is.
	The former aspects should be guided by your specification: essentially this section is a (very) short version of what is typically the first chapter. The latter aspects should be presented as a concise, factual bullet point list. 


	This thesis investigates the solutions for efficient file transfer between a client and a server during an unstable network connection. In particular, CouchDB is used, as it makes use of client-server communication but lacks robustness in the file transfer implementation.

	Currently, CouchDB and most of the other open-source databases can receive and store files as well as structured data. However, the protocols governing the transfer of these files are quite simplistic. Nowadays, the use of mobile devices is prevailing and their storage space is rapidly growing. Big files, like high-definition videos, are being transferred from and to them. However, the network connection they use is not always perfect. A need arises of a new protocol that would minimise the time and traffic used and ensure the successful delivery of large files over the changing or unstable network conditions.

	This project focuses on developing an algorithm that would be able to transfer files of various sizes to and from a CouchDB instance while allowing to pause and resume the transfer, and recover from packet loss during the transfer without the need to restart the transmission from the beginning.

	The main contributions are:

	* An algorithm specification that resolves the said problem and allows resumable file transfer.

	* A Java client and proxy server implementation, which implement the abovementioned algorithm and can be modified to the need of any database implementation.

	* A native CouchDB applicatio of the algorithm.

	* Benchmark on both proxy server and native implementations and comparison between them.


Supporting technologies (1 page).

	This section should present a detailed summary, in bullet point form, of any third-party resources (e.g., hardware and software components) used during the project. Use of such resources is always perfectly acceptable: the goal of this section is simply to be clear about how and where they are used, so that a clear assessment of your work can result.

	* Apache Tomcat server was used in order to test the Java proxy server developed.
	* Apache CouchDB was used as an example database which used the native implementation of the algorithm.

Notation and acronyms (optional, 1-2 pages).

Acknowledgements (1 page).

	Advisor, supervisor, mum, dad, etc.

Contextual Background (10 pages).

	This chapter should describe the project context, and motivate each of the proposed aims and objectives. Ideally, it is written at a fairly high-level, and easily understood by a reader who is technically competent but not an expert in the topic itself.
	In short, the goal is to answer three questions for the reader. First, what is the project topic, or problem being investigated? Second, why is the topic important, or rather why should the reader care about it? For example, why there is a need for this project (e.g., lack of similar software or deficiency in existing software), who will benefit from the project and in what way (e.g., end-users, or software developers) what work does the project build on and why is the selected approach either important and/or interesting (e.g., fills a gap in literature, applies results from another field to a new problem). Finally, what are the central challenges involved and why are they significant?
	The chapter should conclude with a concise bullet point list that summarises the aims and objectives.
	

	Database technology, as a means of storing organized data, is at the core of many systems and services employed in the industry nowadays. Databases are used in conjunction with Database Management Systems (DBMS), which are designed to allow creation, querying, deletion and administration of them. In 1970 Edgar Codd outlined an approach to database construction that was based on relational model. In his paper he suggested a model where a database would consist of interlinked tables of fixed-length records of data, each table used for different type of entity. In this model some bit of information would be used as a “key” to uniquely identify a particular record in different tables. Optional elements would be moved out of the main table where it could then be found by searching with this key.

	Relational model has been the main approach to the database design and implementation until late 2000’s. Up to that point this model was enough to satisfy the industry needs. However, with the increased availability of the internet-connected devices the new industry-wide needs arose. The main trends became: supporting the large number of concurrent users, ability to incorporate semi-structured and unstructured data (e.g. log files, tweets, audio and video), and scaling capabilities. The latter became especially relevant with the rise of cloud computing, where the ability to scale-out with the periodic or spiked loads is crucial. Because of the centralized, share-everything technology of the relational model it scales up (requires a better server, with more CPUs, memory, etc.), rather then out (more servers of the same configuration) and therefore it becomes problematic to use with the applications that demand easy and dynamic scalability.  In order to retrieve data from a relational database the desired information has to be collected from many tables (in case of modern enterprise applications) and combined together before it could be accessed by the application. This became a problem for modern big data applications.

	In 2009 a new approach to database design was invented. The NoSQL idea was introduced, which referred to a movement in database design that didn’t use the conventional relational model. Instead, depending on the implementation, the data was stored in key-value pairs, documents or graphs. For example, a document-oriented NoSQL database takes the data to store and aggregates it into a document using a specified format (e.g. JSON in case of CouchDB). A single document might contain combined data that would span over 20 tables in a relational database. This might lead to duplication, however the storage cost tends to get cheaper and often gets outweighed by the performance advantages. Also, NoSQL databases are designed to be schemaless, which means new types of data can easily be added to documents without application disruption.

	Another prominent trend that had a major impact on the industry is the rise of mobile devices. Mobile phones, tablets and laptops are becoming more popular as the advancements in chip design and manufacturing of these devices decrease their cost and increase the variety. According to comScore mobile usage report number of global users of mobile devices has overtaken the number of users of desktop computers.  Cisco Mobile Data Traffic Forecast states that popularity of the mobile devices continues to rise in terms of data traffic. Over one year, in 2014, the mobile traffic grew by 69% as well as smartphone usage grew by 45%. These figures are forecasted to grow even further by 2019. For example, the report projects that the global mobile traffic will grow nearly tenfold in that period. Another important point mentioned in the report is the dominance of the video traffic in the data transfer – it exceeds 50% of the total mobile traffic.

	However, despite such success of mobile devices their network connectivity is not always the same as their desktop counterparts. The speed, reliability and latency can widely vary depending on the location, environment conditions, distance from the connection point and other factors. Therefore, the developers aiming for mobile market need to consider more efficient transfer protocols. This is especially relevant when mobile operators limit the amount of data that can be used or charge on the amount of data used, as in this case the users want as little network as possible used to complete the task.

	Many NoSQL systems support access by mobile devices. They also allow users to upload external files as attachments to the database entry. In terms of mobile devices these can be videos or pictures. These files can be up to hundreds of megabytes and therefore efficient transfer of them is crucial. Out of the most popular NoSQL database implementations only MongoDB has a specification for storing and retrieving large files from the database called GridFS. However, it is more of a convention of how to store large files, rather than a native implementation. The purpose of this dissertation is to investigate the possible adaptation of a similar file-transfer algorithm to a wider variety of database implementations. 



	TODO: Divide into sub-sections:
		-Introduction and choice of topic (noSQL popularity and mobile networks)
		-The problem of file transfer
		-Objectives?

Technical Background (10 to 20 pages).
	
	This chapter is intended to describe the technical basis on which execution of the project depends. The goal is to provide a detailed explanation of the specific problem at hand, and existing work that is relevant (e.g., an existing algorithm that you use, alternative solutions proposed, supporting technologies).
	Per the same advice in the handbook, note there is a subtly difference from this and a full-blown literature review (or survey). The latter might try to capture and organise (e.g., categorise somehow) all related work, potentially offering meta-analysis, whereas here the goal is simple to ensure the dissertation is self-contained. Put another way, after reading this chapter a non-expert reader should have obtained enough background to understand what you have done, and then accurately assess your work. You might view an additional goal as giving the reader confidence that you are able to absorb, understand and clearly communicate highly technical material.

	1. NoSQL?

	2. CouchDB

	CouchDB is a NoSQL database that has no schema or pre-defined data-structures such as tables. Data in the database is stored in JSON document(s) and therefore the structure of the data (or document) can change depending on the needs.

	2.1 JSON

	JSON is a format that uses human-readable text to transmit data as attribute-value pairs. It is primarily used to transmit data between a server and a web-application. JSON is a language-independent format and many languages have built-in tools to generate and parse such data. 

	JSON accepts various common data types such as Number, String, etc. It also has more complex data types such as Object, which stores an unordered collection of key/value pairs, Array, which stores a list of any other values, and null, which is an empty value.

	2.2 Views

	CouchDB has a special tool designed to query this semi-structured data called view. Views are specified in JavaScript and are the method of aggregating and reporting on the documents in a database. They are built on-demand to aggregate, join and report on database documents. Views are built dynamically and don’t affect the underlying document and there can be many different view representations of the same data.

	2.3 Replication

	CouchDB is a distributed database system. Any number of CouchDB hosts (servers and offline-clients) can have independent “replica copies” of the same database, where applications have the full set of database-interaction features (query, add, edit, delete). When back online or on a schedule, database changes can be replicated bi-directionally. Replication provides redundancy and increases data availability. With multiple copies of the database on separate servers, replication protects a database from the loss of a single server. 

	There are two main replication methods: master to slave and master to master. In the first case there is an original (master) database and its copies (slaves). The changes are submitted to the original and are propagated to its copies. In the second case the updates can be submitted to any database node and will be propagated to all the other nodes. This case is usually more desirable, but it introduces some challenges, like conflicts.

	CouchDB supports master to master replication and therefore sometimes, when database is replicated, a conflict can arise if the same document was changed in both source and destination. CouchDB provides conflict-detection functionality as well as incremental replication, which copies only the documents that were changed since previous replication. 

	3. Related work

	3.1 GridFS for MongoDB

	Similarly to CouchDB, MongoDB is a document-oriented NoSQL database. It uses BSON format of storing data, which is similar to JSON, but is encoded in binary and therefore is not human-readable on its own. Unlike CouchDB, it uses master to slave replication method.

	MongoDB supports binary files as attachments to documents, but limits their size to 16MB. In order to add files larger than 16MB GridFS specification was developed. Instead of storing file in a single document it divides a file into chunks and stores them as a separate document. Another document holds file metadata such as number of chunks, their length, etc. Such file can then be queried using GridFS to retrieve it all or parts of it, by performing range queries.

	3.2 How is this project different?

	While this project is using a similar algorithm to GridFS, it expands on several of its features. The main aim of this project is to create a reliable transfer protocol that can be used to efficiently transfer large file by mobile devices, therefore the differences are:

	•	Project’s proposed algorithm and it’s implementation are designed to tackle the problem of reliable transfer of data and resume the transmission from the point of failure without the need to restart the whole transfer from the beginning. GridFS has no special functionality to be able to cope with network failures.
	•	The implementation is packet and data loss resistant, as it makes sure that each file chunk is received by the server and is intact. The algorithm retries to send the chunk until the acknowledgment from the server arrives. The server does not acknowledge the transfer until the checksum of the data chunk matches the supplied checksum sent by a client.
	•	The transfer can be paused and resumed at any time. This is especially relevant for mobile users, who can use this feature in order to temporarily decrease the network load to use other bandwidth-dependent applications.
	•	A separate implementation of the proposed algorithm in form of a proxy serve is developed in order to be used with any of the similar NoSQL database implementations with minor changes needed to the original implementation. It also can be used with older CouchDB versions that don’t have the proposed chunked transfer functionality. 


Project Execution (20 pages).

	This chapter is intended to describe what you did: the goal is to explain the main activity or activities, of any type, which constituted your work during the project. The content is highly topic-specific, but for many projects it will make sense to split the chapter into two sections: one will discuss the design of something (e.g., some hardware or software, or an algorithm, or experiment), including any rationale or decisions made, and the other will discuss how this design was realised via some form of implementation.
	This is, of course, far from ideal for many project topics. Some situations which clearly require a different approach include:
	• In a project where asymptotic analysis of some algorithm is the goal, there is no real “design and implementation” in a traditional sense even though the activity of analysis is clearly within the remit of this chapter.
	• In a project where analysis of some results is as major, or a more major goal than the implementation that produced them, it might be sensible to merge this chapter with the next one: the main activity is such that discussion of the results cannot be viewed separately.
	Note that it is common to include evidence of “best practice” project management (e.g., use of version control, choice of programming language and so on). Rather than simply a rote list, make sure any such content is useful and/or informative in some way: for example, if there was a decision to be made then explain the trade-offs and implications involved.

	1.	Algorithm design
	1.1	Client algorithm
		*Subject to minor changes*
		The abstract algorithm goes as follows:
		1.	Send a request to the server to start the transfer, containing the document ID of a destination document.
		2.	Wait for server acknowledgment. If an error received or timed out go to 1.
		3.	Construct a stack with chunk numbers for current file.
		4.	For MaxN iterations:
			4.1.	Pop a number N from the stack.
			4.2.	Construct an asynchronous task that would send data from position N*ChunkLength to (N+1)*ChunkLength in the file.
			4.3.	Commence execution of the task.
		5.	While stack is not empty:
			5.1.	If one of the tasks was finished:
				5.1.1.	If the task succeeded:
					5.1.1.1.	Pop a number N from the stack.
					5.1.1.2.	Construct an asynchronous task that would send data from position N*ChunkLength to (N+1)*ChunkLength in the file.
					5.1.1.3.	Commence execution of the task.
				5.1.2.	If the task failed:
					5.1.2.1.	Restart the execution of this task.
			5.2.	If pause signal received:
				5.2.1.	Abort all the tasks.
				5.2.2.	Wait for resume signal.
		6.	Wait for all tasks to finish. If any fail – restart them. 
		7.	Send the final request to indicate that the transfer is finished.
		8.	Wait for server acknowledgment. If an error received or timed out go to 6.

		Explanation:
		In the beginning the algorithm makes sure the server is ready to accept the file chunks. The algorithm then constructs a stack, which will hold the chunk numbers of chunks that haven’t been sent. 
		For the data transfer the algorithm uses asynchronous tasks that are managed by the system. These tasks read a specified part of the file in the memory, construct an HTTP request, send it and wait for the server response. The task fails if the server returned failure or if the timeout is reached. It succeeds in an acknowledgement received. 
		A set number of tasks is initiated in the beginning by popping chunk numbers from the stack and passing the appropriate byte ranges to be read from the file to each task.  Then, while the stack is not empty, the algorithm replaces any finished tasks with new. If the finished task succeeded a new chunk number is popped from the stack and a task instantiated. If the finished task resulted in a failure it’s restarted.
		When the stack is emptied the algorithm waits for all tasks to finish and restart them if necessary. The final request is sent in the end in order to complete the transfer and add the resulting file to a document.
	
	1.2	Chunk format
		Each chunk has a payload and several headers:
			•	“Chunked-Transfer” – can be set to “true” to specify that current request is a part of series and should not be treated on its own, or “final” to end the upload and attach the file to a specified document.
			•	“Start” – the start position of a chunk in the file. Value in bytes.
			•	“MD5” – md5 checksum of the chunk, calculated by the client.
			•	“DocID”. Proxy server only. An ID of a destination document for the file. Since document ID in CouchDB is specified in the URL this header is omitted in native implementation. 

	1.3	

	2. Implementation
	2.1 Proxy server
	2.2 Native implementation
	2.3 Benchmark


Critical Evaluation (10 pages).

	This chapter is intended to evaluate what you did. The content is highly topic-specific, but for many projects will have flavours of the following:
	1. functional testing, including analysis and explanation of failure cases,
	2. behavioural testing, often including analysis of any results that draw some form of conclusion wrt. the aims and objectives, and
	3. evaluation of options and decisions within the project, and/or a comparison with alternatives.
	This chapter often acts to differentiate project quality: even if the work completed is of a high technical quality, critical yet objective evaluation and comparison of the outcomes is crucial. In essence, the reader wants to learn something, so the worst examples amount to simple statements of fact (e.g., “graph X shows the result is Y”); the best examples are analytical and exploratory (e.g., “graph X shows the result is Y, which means Z; this contradicts [1], which may be because I use a different assumption”). As such, both positive and negative outcomes are valid if presented in a suitable manner.

	*Benchmark?
	*GridFS vs my algorithm

Conclusion (2 pages).
	
	The concluding chapter of a dissertation is often underutilised because it is too often left too close to the deadline: it is important to allocation enough attention. Ideally, the chapter will consist of three parts:
	1. (Re)summarise the main contributions and achievements, in essence summing up the content.
	2. Clearly state the current project status (e.g., “X is working, Y is not”) and evaluate what has been achieved with respect to the initial aims and objectives (e.g., “I completed aim X outlined previously, the evidence for this is within Chapter Y”). There is no problem including aims which were not completed, but it is important to evaluate and/or justify why this is the case.
	3. Outline any open problems or future plans. Rather than treat this only as an exercise in what you could have done given more time, try to focus on any unexplored options or interesting outcomes (e.g., “my experiment for X gave counter-intuitive results, this could be because Y and would form an interesting area for further study” or “users found feature Z of my software difficult to use, which is obvious in hindsight but not during at design stage; to resolve this, I could clearly apply the technique of Smith [7]”).

	1. Main achievements
	2. Further study.

Bibliography.

Appendices.

	Content which is not central to, but may enhance the dissertation can be included in one or more appendices; examples include, but are not limited to
	• lengthy mathematical proofs, numerical or graphical results which are summarised in the main body,
	• sample or example calculations, and
	• results of user studies or questionnaires.
	Note that in line with most research conferences, the marking panel is not obliged to read such appendices.